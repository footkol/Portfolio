{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04546118-9eac-4438-9bd3-b0c1132ae05f",
   "metadata": {},
   "source": [
    "This function processes data for multiple countries. It reads input CSV files, drops unnecessary columns, filters rows based on a condition, calculates totals, removes duplicates, reads another CSV file, and merges the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4d16db-7c15-4bfd-a8e6-546eb757eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_country_data():\n",
    "\n",
    "    countries = ['SP', 'UK', 'DE', 'DK', 'SE', 'HU', 'IT', 'PO', 'NE']\n",
    "\n",
    "    for country in countries:\n",
    "        file_path = os.path.join('../data', f'total_green_{country}.csv')\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        # Dropping unnecessary columns\n",
    "        columns_to_drop = ['EndTime', 'UnitName', 'AreaID']\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Defining the output CSV file name and a common column\n",
    "        load_csv_file = f'../data/load_{country}.csv'\n",
    "        common_column = 'StartTime'\n",
    "\n",
    "        # Condition to filter rows based on 'PsrType'\n",
    "        condition = df['PsrType'].isin(['B01','B09', 'B10', 'B11', 'B12', 'B13', 'B15', 'B16', 'B18', 'B19'])\n",
    "        \n",
    "        filtered_df = df[condition].copy()\n",
    "    \n",
    "        # Calculating the total green energy  \n",
    "        filtered_df['total'] = filtered_df.groupby('StartTime')['quantity'].transform('sum')\n",
    "\n",
    "        # Removing duplicate rows after combining total green energy\n",
    "        filtered_df = filtered_df.drop_duplicates(subset='StartTime') \n",
    "\n",
    "        # Merging load energy with total green energy for each country\n",
    "        load_df = pd.read_csv(load_csv_file)\n",
    "        load_df = load_df.drop(columns=columns_to_drop)\n",
    "        load_df = pd.merge(load_df, filtered_df, on=common_column, how='left')\n",
    "        \n",
    "        output_path = '../data'\n",
    "        load_df.to_csv(f'{output_path}/total_{country}.csv', index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b306aa69-f006-4ded-baa7-b7e918ffd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_country_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32236b78-c273-4c27-8af7-59be48dca5f9",
   "metadata": {},
   "source": [
    "This function normalizes data for multiple countries.  It homogenizes date stamps to 1-hour intervals for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df80955c-6d71-495e-af53-2d88835d6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_hour():\n",
    "    \n",
    "    countries = ['SP', 'UK', 'DE', 'DK', 'SE', 'HU', 'IT', 'PO', 'NE']\n",
    "\n",
    "    for country in countries:\n",
    "        \n",
    "        file_path = os.path.join('../data', f'total_{country}.csv')\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        # Removing unnecessary parts and converting to to datetime format\n",
    "        df['StartTime'] = df['StartTime'].str[:-7].str.replace('T', ' ')\n",
    "        df['StartTime'] =  pd.to_datetime(df['StartTime'])\n",
    "    \n",
    "        df.set_index('StartTime', inplace=True)\n",
    "\n",
    "        # Resampling the DataFrame to hourly frequency\n",
    "        df['total'] = df.resample('1H').sum()['total']\n",
    "        df['load_norm'] = df.resample('1H').sum()['Load']\n",
    "    \n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Filtering rows where the minute part of 'StartTime' is 0\n",
    "        df = df[df['StartTime'].dt.minute == 0]\n",
    "    \n",
    "        df.rename(columns={'total': f'green_energy_{country}', \n",
    "                         'load_norm': f'{country}_Load'}, inplace=True)    \n",
    "    \n",
    "         # Dropping unnecessary columns\n",
    "        columns_to_drop = ['Load', 'PsrType', 'quantity' ]\n",
    "        df = df.drop(columns=columns_to_drop)       \n",
    "\n",
    "        df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "        output_path = '../data'\n",
    "        df.to_csv(f'{output_path}/norm_{country}.csv', index=False)\n",
    "\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e3deb1-b04a-42a4-afc0-d896a49b1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_by_hour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9476189-aa34-47e4-bb30-9db10f90d369",
   "metadata": {},
   "source": [
    "This function merges normalized data from multiple countries into a single DataFrame. It reads normalized CSV files, merges them based on the 'StartTime' column using an outer join, interpolates missing values using linear interpolation, and writes the merged data to a new CSV file named 'merged_data.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff8f336-fbe9-4de0-b9a6-4410b679e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files():\n",
    "\n",
    "    countries = ['SP', 'UK', 'DE', 'DK', 'SE', 'HU', 'IT', 'PO', 'NE']\n",
    "\n",
    "    for country in countries:\n",
    "        \n",
    "        # Initializing an empty DataFrame for the result\n",
    "        result_df = pd.DataFrame()\n",
    "    \n",
    "        for country in countries:\n",
    "            file_path = os.path.join('../data', f'norm_{country}.csv')\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Assigning results to the first DataFrame\n",
    "            if result_df.empty:\n",
    "                result_df = df\n",
    "            else:\n",
    "                # Merging the current DataFrame with the result DataFrame on the 'StartTime' column\n",
    "                result_df = pd.merge(result_df, df, on='StartTime', how = 'outer')\n",
    "        \n",
    "        result_df.set_index('StartTime', inplace=True, drop = True) \n",
    "                \n",
    "        result_df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "    \n",
    "        output_path = '../data'\n",
    "        result_df.to_csv(f'{output_path}/merged_data.csv', index = True)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a301a2e2-e689-462d-8da0-d0eadd53d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1062459-daac-4ad2-b0c5-e4971e3b1972",
   "metadata": {},
   "source": [
    "This function creates a separate file for each country containing surplus of green energy and country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91689250-9c76-4d5e-b97b-fb571ee95e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_surplus():\n",
    "    \n",
    "    countries = ['SP', 'UK', 'DE', 'DK', 'SE', 'HU', 'IT', 'PO', 'NE']\n",
    "\n",
    "    for country in countries:\n",
    "        \n",
    "        file_path = os.path.join('../data', f'total_{country}.csv')\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "        df['StartTime'] = df['StartTime'].str[:-7].str.replace('T', ' ')\n",
    "        df['StartTime'] =  pd.to_datetime(df['StartTime'])\n",
    "    \n",
    "        df.set_index('StartTime', inplace=True)\n",
    "    \n",
    "        df['total'] = df.resample('1H').sum()['total']\n",
    "        df['load_norm'] = df.resample('1H').sum()['Load']\n",
    "    \n",
    "        df = df.reset_index()\n",
    "    \n",
    "        df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "        df = df[df['StartTime'].dt.minute == 0]\n",
    "    \n",
    "        columns_to_drop = ['Load', 'PsrType', 'quantity' ]\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Calculating 'surplus' and adding 'country' column\n",
    "        df['surplus'] = df['total'] - df['load_norm']\n",
    "        df['country'] = f'{country}'    \n",
    "        output_path = '../data'\n",
    "        df.to_csv(f'{output_path}/norm_surp_{country}.csv', index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d59f7a-cb05-4f53-afca-8a9f9d6f2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_surplus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec211378-b78c-44ed-a0cd-ab0e2fb3d0d6",
   "metadata": {},
   "source": [
    "This function normalizes surplus data for multiple countries. In addition, it creates a separate column containing ID code for the country with the biggest surplus of green energy.\n",
    "\n",
    "The The country IDs are as following\r\n",
    "- Spain: SP 0\r\n",
    "- United Kingdom: UK 1\r\n",
    "- Germany: DE 2 \r\n",
    "- Denmark: DK 3\r\n",
    "- Sweden: SE 4\r\n",
    "- Hungary: HU 5\r\n",
    "- Italy: IT 6\r\n",
    "- Poland: PO 7\r\n",
    "- Netherlands: NE 8\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd0baae-71a0-4809-b1ea-337957aaada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surplus_calc():\n",
    "    \n",
    "    file_names = ['SP', 'UK', 'DE', 'DK', 'SE', 'HU', 'IT', 'PO', 'NE']\n",
    "    \n",
    "    dfs = {}\n",
    "    \n",
    "    for i, country_code in enumerate(file_names):\n",
    "        file_path = f'../data/norm_surp_{country_code}.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs[f'df{i}'] = df\n",
    "    \n",
    "    # Concatenating the 'surplus' columns from all DataFrames\n",
    "    df_new = pd.concat([df['surplus'] for df in dfs.values()], axis=1, join='outer') \n",
    "    \n",
    "    df_new.columns = [f'surplus_{i}' if i < 9 else 'max_value' for i in range(9)]\n",
    "    \n",
    "    max_column_index = df_new.idxmax(axis=1)\n",
    "    \n",
    "    # Finding the maximum value in each row\n",
    "    max_value = df_new.max(axis=1)\n",
    "    \n",
    "    # Creating a new column to store the maximum value\n",
    "    df_new['max_value'] = max_value\n",
    "    \n",
    "    # Creating a new column to store the location of the maximum value\n",
    "    df_new['max_column'] = max_column_index\n",
    "    \n",
    "    start_time_column = dfs['df0']['StartTime']\n",
    "    df_new['StartTime'] = start_time_column\n",
    "       \n",
    "    df_new['max_column'] = df_new['max_column'].str[-1].astype(int)\n",
    "    df_new.head()\n",
    "    df_new.to_csv('../data/surplus.csv', index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15629f44-8c61-4c6c-b4a1-6858c38e86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "surplus_calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04710ee0-90c5-42a0-b165-e62e0f4ae305",
   "metadata": {},
   "source": [
    "This function combines surplus data with merged data into the final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726ecb8c-ee2c-4909-88f2-a0f4f7aebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_merge():\n",
    "    \n",
    "    df1 = pd.read_csv('../data/merged_data.csv')\n",
    "    df2 = pd.read_csv('../data/surplus.csv')\n",
    "    \n",
    "    df = pd.merge(df1, df2[['StartTime', 'max_column']], on='StartTime', how='inner')\n",
    "    df.rename(columns={'max_column': 'label'}, inplace=True)   \n",
    "    \n",
    "    \n",
    "    df.to_csv(f'../data/final_data.csv', index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25d6f43-4b58-4d36-8e3f-ae44e7cbe8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13049524-982d-45b2-8e02-af6a44ccac18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
